import time
import regex
import functools
import tldextract
import emoji
import gcld3
import requests
from urllib.parse import urlparse
from ast import literal_eval
from nltk import word_tokenize
from nltk.util import ngrams
from nltk.corpus import stopwords
from dateutil import tz
from dateutil import parser
from geopy.geocoders import Nominatim

to_zone = tz.tzlocal()

detector = gcld3.NNetLanguageIdentifier(min_num_bytes=10, max_num_bytes=1000)

# Unshorten URLs with unshrtn
@functools.lru_cache
def unshrtn(url):
    resp = requests.get("http://localhost:3000", params={"url": url}).json()
    for key in ["canonical", "long", "short"]:
        if key in resp:
            return (resp[key], resp["status"], resp["title"])
    return None


# Do a cached geocode lookup via Nominatim
@functools.lru_cache
def geo(column):
    geolocator = Nominatim(user_agent="SIO")
    if column and column != "null":
        location = geolocator.geocode(column, timeout=5, language="en")
        if location is not None:
            latlong = str(location.latitude) + " " + str(location.longitude)
            return (latlong, location.address, location.address.split(",")[-1].strip())


# Convert to local time
def ltime(column):
    return parser.parse(column).astimezone(to_zone)


# Strip HTML tags
def nohtml(column):
    tag_re = regex.compile(r"(<!--.*?-->|<[^>]*>)")
    return tag_re.sub("", column)


# Convert abbreviated numbers like 1.1k to actual numbers
def unk(x):
    if x.isdigit():
        return int(x)
    else:
        if len(x) > 1:
            if x[-1] == "k":
                y = float(x[:-1]) * 1000
            elif x[-1] == "m":
                y = float(x[:-1]) * 1000000
    return int(y)


# Convert emoji to :this: representation, in memory of Chloe Price
def noemoji(column):
    text = emoji.demojize(column)
    return text


# Extract all emoji into a list
def gemoji(column):
    emoji_list = []

    if column:
        data = regex.findall(r"\X", column)
        for word in data:
            if any(char in emoji.UNICODE_EMOJI_ALIAS_ENGLISH for char in word):
                emoji_list.append(word)

    return emoji_list


# Extract all emoji into a text-based list
def temoji(column):
    emoji_list = gemoji(column)
    temoji_list = []
    for char in emoji_list:
        temoji = emoji.demojize(char)
        temoji_list.append(temoji)
    return temoji_list


# Twitter helper functions
def hashtags(column):
    try:
        return [hashtag["tag"] for hashtag in column["hashtags"]]
    except:
        return None


def mentions(column):
    try:
        return [name["username"] for name in column["mentions"]]
    except:
        return None


def urls(column):
    results = []
    try:
        for url in column["urls"]:
            if "unwound_url" in url:
                results.append(url["unwound_url"])
            elif url.get("expanded_url"):
                results.append(url["expanded_url"])
            elif url.get("url"):
                results.append(url["url"])

        return results

    except:
        return None


def domains(column):
    if column:
        return tldextract.extract(column).registered_domain


# Sometimes we get data in a CSV or something that's a literal string,
# but with array notation.
def strtoarray(column):
    if column:
        return literal_eval(column)


# These re- functions are for plain text fields where we don't have
# these as separate metadata/columns
def rementions(column):
    return regex.findall(r"\B@\w\w+", column)


def rehashtags(column):
    return regex.findall(r"\B#\w\w+", column)


def reurls(column):
    r = r"(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'\".,<>?«»“”‘’]))"
    if column:
        url = regex.findall(r, column)
        return [x[0] for x in url]
    else:
        return ""


# Normalize URLs for cleaner freq analysis
def normurl(column):
    if column:
        url = urlparse(column)[1:]
        url = "".join(url)
        if url.endswith("/"):
            url = url[:-1]
        if url.startswith("www."):
            url = url[4:]

        return url


# Sloppy ngram filter
def grams(column, n):
    if column:
        stop = stopwords.words("english")
        stop.extend(["http", "https"])
        w = word_tokenize(column)
        alnum = [word for word in w if word.isalnum()]
        clean = [word for word in alnum if not word in stop]
        tokens = ngrams(clean, n)
        return [" ".join(thegrams) for thegrams in tokens]


@functools.lru_cache
def ld(column):
    l = ""
    if column:
        detect = detector.FindLanguage(text=column)
        if detect.is_reliable:
            l = detect.language
    return l


# Options
options.disp_column_sep = "│"
options.disp_keycol_sep = "║"
options.confirm_overwrite = False
options.clipboard_copy_cmd = "pbcopy"
options.clipboard_paste_cmd = "pbpaste"
options.color_default = ""
options.quitguard = True
options.color_key_col = 110
options.color_note_type = 8
options.scroll_incr = -3
options.some_selected_rows = True
options.input_history = "previnputs"

# Keybindings
bindkey("0", "go-leftmost")
bindkey("4", "go-rightmost")

# Whether you want binning is pretty context-specific, so these let
# you switch modes
Sheet.addCommand("b", "nobinning", "options.numeric_binning=False")
Sheet.addCommand("B", "binning", "options.numeric_binning=True")

# I keep needing to switch date formats, so make z@ do a "zoom" and show date+time
Sheet.addCommand(
    "@", "shortdate", 'options.disp_date_fmt = "%Y-%m-%d"; cursorCol.type=date'
)
Sheet.addCommand(
    "z@", "longdate", 'options.disp_date_fmt = "%Y-%m-%d %H:%M"; cursorCol.type=date'
)

# vim: syntax=python
